
# 2章 ユーザモードで実現する機能

ユーザモードからカーネルモードの処理が必要なときは **システムコール** を介して呼ばれる．

## システムコール

システムコールは以下のような操作を担う：

- プロセス生成/削除
- メモリ確保/解放
- プロセス間通信
- ネットワーク処理
- ファイルシステム操作
- ファイル操作（デバイスへのアクセス）


### CPUのモード遷移

システムコールはCPUの特殊な命令を実行することで発行する．システムコールが発行されると割り込みが発生し，ユーザモードからカーネルモードに遷移して該当する処理が走り，その処理が終わると再びユーザモードに戻る．カーネルモードの開始時にはプロセスからの要求が正当であるかがチェックされ，不正なら失敗させる．（もしプロセス自身がモードを切り替える手段を持っているなら権利の設計としておかしいため，このような仕組みになっている）．


### システムコール呼び出しの様子

`strace -o 〈Log〉 〈Command〉`  で，通常のコマンド `〈Command〉` による処理に対して発行されたシステムコールのログが `〈ログファイル〉` に出力される．例えばHello Worldプログラムでは `write()` が発行されている様子がわかる（ほかにも前後にプロセスの開始処理と終了処理に伴うシステムコールがたくさん発行される）．


### 実験

**`sar` コマンド** で各CPUコアがユーザモードとカーネルモードのどちらで実行しているかの割合を見ることができる．


### システムコールの所要時間

`strace` の `-T` オプションを使うと各システムコールの処理に何マイクロ秒かかっているのかを出力できる．


## システムコールのラッパー関数

システムコールは（CPUの実装に依存することから）アーキテクチャ依存のアセンブリコードを使って呼び出す必要があり，C言語などの或る程度高級な言語からはこれをラップして “FFI的に” 呼び出せるようにしてある（OSがライブラリを提供している）．


## 標準Cライブラリ

C言語にはISOが定めた標準ライブラリがあり，Linuxもこれに準拠してライブラリを提供している．通常はGNUの `glibc` で，これがシステムコールのラッパーを含む．

プログラムがどんなライブラリをリンクしているかは **`ldd` コマンド** で確かめることができるので見てみるとよい．


## OSが提供するプログラム

OSはライブラリだけでなくプログラムも提供しており，（よく知っているように）以下のようなものがある：

- システムの初期化： `init`
- OSの挙動変更： `sysctl`，`nice`，`sync`
- ファイル操作： `touch`，`mkdir`
- テキスト処理： `grep`，`sort`，`uniq`
- 性能測定： `sar`，`iostat`
- コンパイラ： `gcc`
- インタプリタ： `python`，`ruby`
- シェル： `bash`
- ウィンドウシステム： X


# 3章 プロセス管理

カーネルによるプロセス生成/削除の機能について触れる．実際のLinuxに於けるプロセス生成/削除には **仮想記憶** という概念が関わっており，これは5章で触れる．ここでは仮想記憶がない単純な場合を扱う．


## 2段階のプロセス生成

プロセス生成には2つの異なる目的がある：

- 同じプログラムによる処理を複数走らせる
- 全く別のプログラムを走らせる

これらそれぞれの目的に対して  **`fork()`** と **`execve()`** という2つのC言語の関数があり，これらは内部的に `clone()` と `execve()` というシステムコールを発行する．


## `fork()` 関数

`fork()` の呼び出しは次のような流れでプロセスを生成する：

1. 子プロセス用のメモリ領域を確保し，親プロセスのメモリの内容をそこにコピーする．
2. 親プロセスと子プロセスとで違うコードを実行するように分岐する．これは `fork()` の戻り値が親と子で異なるものになることを利用する．


## `execve()` 関数

全く別のプログラムによるプロセスを生成する場合は以下の流れによる：

1. 実行ファイルを読み出してプロセスのメモリマップに必要な情報を得る．
2. 現在のプロセスのメモリを新しいプロセスのデータで上書きする．
3. 新しいプロセスでプログラムを最初の命令から実行する．

つまり，プロセスが増えるのではなく既存のプロセスが書き換わる．

Linuxの実行ファイルは **ELF** (**Executable Linkable Format**) という形式で記録されている．**`readelf` コマンド** を使うと実際の実行ファイルにどんなメタデータ，コード領域，データ領域が含まれているのかを見ることができる．

実際にプログラムが走っている際のメモリマップは，`cat /proc/〈PID〉/maps` で見ることができる．

全く新しい別プロセスを生成する場合は，まず `fork()` で子プロセスを生成し，その後に `execve()` を呼んでプログラムを変更するという手順をとる．例えば `bash` が `echo` を呼び出すときも，まず `bash` の子プロセスをつくり，それを `echo` にプログラムに書き換えて実行を開始する．


## 終了処理

プログラムの終了は `_exit()` 関数が使われる．これは内部的に `exit_group()` システムコールを発行する．Linuxはこれによりそのプロセスに割り当てられていたメモリを回収する．

ユーザランドでは通常はこの関数を直接呼び出さず，そのラッパ的関数である `exit()` を呼び出す．C言語製の実行ファイルは `main()` が戻り値を返した時も同様の処理が走るようにコンパイルされている．


# 4章 プロセススケジューラ

この章では複数のプロセスが “同時に走っている” ようにするために必要な仕組みについて実験する．

マルチコアCPUでは1つのコアが1つの **論理CPU** という単位でLinuxに認識される．なお，**ハイパースレッド機能** が有効な場合は各コア内のハイパースレッドが論理CPUとして認識される（6章参照）．


## 実験プログラムの仕様

ひたすらCPU時間を使う処理をして有限時間で終わるプロセスを1個以上同時に動かして，全てが完了するまで以下を観察する：

- 或る時点で各論理CPU上でどのプロセスが動作しているか
- それぞれの進捗はどれだけか

実験プログラムは以下の引数をとる：

- `n`： 同時に動かすプロセス数
- `total`： プログラムを動作させる合計時間
- `resol`： 統計情報の採取間隔

（実際の実装が掲載されている）


## 実験

`n={1,2,4}`，`total=100`，`resol=1` で実験して比較する．結果は元の本参照（1論理CPU内では普通に並行していることが確かめられる）．


## 考察

以下のことがわかる：

- 1つの論理CPU上で各瞬間に動作できるプロセスは1つだけ
- 論理CPU上ではプロセスを順番にラウンドロビン方式で実行している
- 各プロセスのタイムラプスは大体等しい
- 全プロセス終了までの所要時間はプロセス数に比例して増加する


## コンテキストスイッチ

論理CPU上で動作するプロセスが切り替わるときにはコンテキストスイッチという処理が行なわれる．


## プロセスの状態

プロセスには次のような状態がある（網羅しているわけではないが，以下が典型的）：

- **実行状態**： 現在論理CPUを使っている
- **実行待ち状態**： CPU時間が割り当てられるのを待機している
- **スリープ状態**： 何らかのイベントが発生するのを待機しており，イベント発生まではCPU時間を使わない
  * イベントとはキーボードなどによるユーザ入力とか，タイマーの発火とか，ディスクへの書き込み完了とか，ネットワークを介した送受信の完了といったもの．
- **ゾンビ状態**： プロセスとして終了した後であり，親プロセスが終了状態を受け取るのを待っている

実際に `ps ax | wc -l` を叩くとわかるが，何も起動していないように見えても実際には300件くらいのプロセスが動作している．上記の実験で他のプロセスの影響がなかったのは，それらがスリープ状態だったため．各プロセスがどの状態なのかは `ps ax` の出力結果の3列目を見るとわかる．

- `R`： 実行状態/実行待ち状態．
- `S` または `D`； スリープ状態．後者はシグナルによって実行状態に戻ったりはしないもので，ディスクへのアクセスなどが該当する．
- `Z`： ゾンビ状態．


## 状態遷移

プロセスは或る意味では “有限状態機械” で，以下のように状態遷移する：

```
開始
 |      CPU実行権を得る
 |       +--------+
 |       |        |
 V       |        V       終了処理を呼ぶ             親プロセスが終了
[実行可能状態]     [実行状態]------------->[ゾンビ状態]-------------->[終了]
 ^       ^        |    |
 |       |        |    |
 |       +--------+    |
 |      CPU実行権を失う  |
 |                     |
 +----[スリープ状態]-----+
イベント発生      イベント待ち
```


## アイドル状態

論理CPU上でどのプロセスも実行状態でない期間は，**アイドルプロセス** という何もしない特殊なプロセスが動作する．このプロセスはCPUの特殊な命令を用いて論理CPUを休止状態（**アイドル状態**）にし，休止状態のCPUは1つ以上のプロセスが実行可能状態になるまで待機する．なんのプログラムも動かしていないと消費電力が抑えられるのはこのため．

論理CPUが単位時間あたりでどの程度アイドル状態にあったかは `sar -P ALL 1` などで確認できる．実際，`while (1)` などで無限ループするプログラムを動かすとアイドル状態が0になることが実験的に確かめられる．


## スループットとレイテンシ

- **スループット**： 単位時間あたりの総仕事量（高いほど良い）
- **レイテンシ**： 各処理の開始から終了までの経過時間（短いほど良い）

演繹的には明らかに以下が言える：

- 全ての論理CPUがアイドル状態にならない状況下では，プロセスを増やしてもスループットは変わらない．
- プロセスを増やすほどレイテンシは悪化する．


## 実際のシステム

スループットとレイテンシはトレードオフになりがち．プログラムを書くときは，望ましい目標を定義し，`sar` を使って測定することでチューニングしたりする．


## 論理CPUが複数の場合のスケジューリング

論理CPUが複数ある場合は，**ロードバランサ** または **グローバルスケジューラ** と呼ばれる機能が動作し，各論理CPUにプロセスを公平に分配する役割を担う．


## 実験方法

論理CPUが複数ある場合の動作は，**`taskset` コマンド** で実際にCPUを指定して行える．これは内部で `sched_setaffinity()` というプロセスを特定の論理CPUに制限するシステムコールを呼んでいる．


## 実験結果

2つのプロセスを2つの論理CPUで走らせると，それぞれ真に同時に走っていることが確かめられる．また，4つのプロセスを2つの論理CPUで走らせると，各論理CPUには2つずつプロセスが割り振られ，独立に両CPU上でそれぞれ2つのプロセスが切り替わりながら走る様子もわかる．


## 優先度の変更

OSは原則としてプロセスに対し平等にスケジューリングを行なうが，特定のプロセスの実行の優先度を高く設定することもでき，そのために **`nice()`** というシステムコールが用意されている．これはプロセスに−19から20までの整数で実行優先度を与えるもので，デフォルトは0であり，大きいほど優先度が高い．優先度が高いプロセスほどCPU時間を多く割り振るようにスケジュールされる．優先度を下げるのは権限に制約がないが，優先度を上げるにはroot権限を要する．


# 5章 メモリ管理

Linuxはシステムに搭載されている全メモリをカーネルの **メモリ管理システム** と呼ばれる機能により管理する．


## メモリに関する統計情報

**`free` コマンド** により，システムが搭載しているメモリの量と現在使用中のメモリの量を見ることができる．出力は以下のような項目をもつ：

- `total` フィールド： システムが搭載している全メモリ量．
- `free` フィールド： 見かけ上の空きメモリ量
- `buff/cache` フィールド： **バッファキャッシュ** と **ページキャッシュ**（6章参照）が使っているメモリ量．これらは空きメモリが減ってきたらカーネルによって解放される．
- `available` フィールド： 実質的な空きメモリ量．`free` の値に解放可能なカーネル内メモリ領域の量を足したもの．


## Out Of Memory

メモリ使用量が増えてくると，メモリ管理システムはまずカーネル内の解放可能なメモリ領域を解放する．その後もメモリ使用量が増えてどうしてもメモリが確保できなくなった場合，OSは **Out Of Memory**（**OOM**）という状態になる．この状態に至ると，メモリ管理システムは適切にプロセスを選んで強制終了することでメモリを解放するという最終手段を取る．

業務用のサーバなどでは，OOMが生じたときにどのプロセスが強制終了させられたかわからない状態で稼働し続けるのは困るので，OOMが発生するとシステムを強制終了するという設定にすることがある（`sysctl` の `vm.panic_on_oom` パラメータを `1` に設定すると可能）．


## 単純なメモリ割り当て

Linuxでのメモリ割り当ての仕組みには仮想記憶というものが欠かせないが，ここでは仮想記憶を加味しない単純な仕組みを説明し，その問題点を挙げる（この問題点を解決するのがまさに仮想記憶）．

カーネルがプロセスにメモリを割り当てるのは以下の2つのタイミングがある：

- プロセス生成時（これは3章で見た）
- プロセス生成後，追加で動的に確保するとき（これがこの章の内容）

プロセスは，追加でメモリが必要になったらメモリ獲得用のシステムコールを発行することでカーネルに確保を要請する．カーネルはそれをうけて必要な量のメモリを割り当てて，その先頭アドレスを返す．

この単純な方法には以下のような問題点がある：

- 断片化が進むと新たな確保に支障が出る
- 別用途のメモリにアクセスできてしまう
  * メモリのアドレスが丸裸なので，プロセスが自分用でないメモリにアクセスできてしまうなど抽象化に漏れがある．特にカーネルのメモリ領域にアクセスできるとまずい．
- マルチプロセスの扱いが困難
  * 各プログラムに於いてコードが何番地，データが何番地と決まった位置にあることを想定するように書かれていると，同じプログラムでもうひとつプロセスを立ち上げたりすると不整合が起きる．2種類以上のプログラムを共存させる場合も，使用するメモリの番地が重複するとやはり不整合が起きる．


## 仮想記憶

前節の問題点を解消するため，現代的なCPUには **仮想記憶** という仕組みが備わっており，これによりプロセスから見える **仮想アドレス** を実際の物理的なメモリの番地である **物理アドレス** にマップすることで抽象化をはかっている．プロセスから直接物理アドレスにアクセスする方法は存在しないようになっている．

このアドレス変換は実際にはカーネルが使うメモリ内に保持されている **ページテーブル** という表を引くことで行なう（CPUがカーネルを介さずに直接見る）．仮想記憶ではメモリはページ単位で管理されており，アドレス変換もページ単位で行なわれる．ページテーブル中の1つのページに対応するデータを **ページテーブルエントリ** と呼ぶ．ページテーブルの大きさはアーキテクチャごとに異なり，例えばx86\_64では4KB．

ページテーブルはページ単位で仮想アドレスごとにそれに紐づく物理アドレスがどこか，或いは存在しないかを保持しており，存在しないものを引いた場合は **ページフォルト** という割り込みが発生する．これに対してはページフォルトハンドラという処理が動き，この処理の中でプロセスによるメモリアクセスが不正だったことを検知する．その後 **SIGSEGV** というシグナルによりプロセスにその旨を通知し，通常その通知を受けたプロセスは強制終了させられる．

仮想記憶により以下の恩恵が得られる：

- 仮想アドレスでは連続していても物理アドレスではバラバラなように割り振ることができるので，断片化の問題が生じない．
- 仮想アドレス空間はプロセスごとに独立に確保されるので，別のプロセスやカーネルのメモリにアクセスすることはできず抽象化に漏れがない．
  * カーネルのメモリは，実装上の都合により実は全てのプロセスの仮想アドレス空間にマップされている．ただし，これはカーネルモードで動作しているときにしか読めないようになっているため，ユーザモードからカーネルのメモリを見たり変更したりすることはできない．
- マルチプロセスの取り扱いも同様の理由により困難が生じない．


## 仮想記憶の応用

以下のような機能が仮想記憶を応用して実現されている：

- **ファイルマップ**
- **デマンドページング**
- **コピーオンライト方式** の高速なプロセス生成
- **スワップ**
- **階層型ページテーブル**
- **ヒュージページ**


## ファイルマップ

通常プロセスがファイルにアクセスするときはファイルを開いた後に `read()`，`write()`，`lseek()` などのシステムコールを発行して行なうが，これに加えてLinuxにはファイルの領域を仮想アドレス空間上にマップするという機能がある．

マップされたファイルは，メモリアクセスと同じ方法でアクセスできる．writeなどの変更が施された領域は，所定のタイミングでストレージ上のファイルに書き戻される（6章参照）．


## デマンドページング

既に述べたように，プロセスに対するメモリ割り当ては次のような段階を経る：

- カーネルが，必要な領域を物理メモリ上に確保する
- カーネルがページテーブルを設定し，仮想アドレスを物理アドレス空間に紐づける

しかし，この方法だとメモリ消費に無駄が生じる．なぜなら，プロセスが確保したメモリには当分使われなかったり或いは終了まで使われない領域もあるため．例えばglibcは `malloc()` が呼び出されるよりも前にあらかじめ一定量のメモリを確保しておいて `malloc()` が呼び出されたらそこから割り当てて仮想アドレスを返すという動作をするが，実際にユーザが `malloc()` などで確保しなかったらあらかじめ確保していた分は無駄である．これを解決するのが **デマンドページング** の仕組み．

デマンドページングは，“物理アドレスを割り当てずに仮想アドレスを確保してそれをプロセス側に通知しておき，その仮想アドレスに最初にアクセスがあったときに実際に物理アドレスを確保する” という仕組み．したがってページテーブルの要素は「未割り当て」「割り当て済みで，物理メモリも確保済み」に加えて「割り当て済みだが物理メモリは未確保」という状態をとるようになる．

実際の動作としては，CPUがまずページテーブルを見て仮想アドレスが物理アドレスと紐づいていないことを検知してページフォルトを起こす．そしてカーネルのページフォルトハンドラが仮想アドレスに物理メモリを新規に割り当ててページテーブルを書き換える．そしてユーザモードに戻って通常通り処理が継続される．

デマンドページングが行なわれている下では，メモリ確保は成功したがその仮想アドレスへの最初のアクセスのときにメモリ枯渇で物理メモリが確保できず死ぬということが起きうることに注意．

デマンドページングの挙動は，実際にメモリを確保してその後アクセスするという処理を走らせて `sar` コマンドで仮想メモリと物理メモリの使用量を見ると実験的にも確かめられる．

メモリの枯渇は，物理メモリの枯渇のほかに仮想メモリの枯渇もある．これは仮想メモリのアドレス空間の範囲いっぱいまで使い切ってしまったことに相当し，物理メモリが残っていても発生する．x86では仮想メモリのアドレス空間が4GB程度しかなかったためによく起きていた．x86\_64だと128TB程度あるので今のところまず起きない（いつかはよく起きる時代になるかも）．


## コピーオンライト

`fork()` も仮想記憶の仕組みを使って高速化されている．`fork()` の発行時には，親プロセスのメモリを全て子プロセス用にコピーするのではなく，ページテーブルだけを複製し，物理メモリは共有する（つまり，ポインタを複製しているのと同じ要領）．しかし，どちらかが書き込みを行なおうとすると共有は破綻するはず．そのため，以下のような **コピーオンライト** （**CoW**）という方法をとる：


- 最初に親プロセスも子プロセスも書き込み権限がないように設定しておく．
- 親プロセスか子プロセスのどちらかかページ中のどこかを更新しようとしたとき，以下の流れで共有を解除する：
  * ページへの書き込み権限がないため，ページフォルトが生じる．
  * CPUがカーネルモードに移行してページフォルトハンドラが動き出す．
  * ページフォルトハンドラはアクセスされたページを別の場所にコピーし，書き込もうとしたプロセスに割り当てた上で内容を書き換える．
  * 親プロセスと子プロセスのそれぞれについて，共有が解除されたページに対応するページテーブルエントリを更新する．
    + 書き込んだ側のプロセスは，新たに与えられた物理ページと紐づけた上で書き込み権限を与えられる．
    + 他方のプロセスも（ページテーブルエントリは旧来のものを指したままで変わらず）書き込み権限を与えられる．

これもやはり `fork()` には成功するが共有解除のタイミングで物理メモリ枯渇による失敗が生じうることに注意．


## スワップ

物理メモリが枯渇するとOOMという状態になると既に述べたが，実際には救済措置として **スワップ** という機能がある．これはストレージデバイスをメモリの一部として一時的に使用する仕組み．枯渇が発生しそうになったら，メモリの一部をストレージに退避させることによってメモリ枯渇を防ぐ．このときにメモリを退避するためのストレージの領域を **スワップ領域** と呼ぶ．

物理メモリが枯渇しそうになったとき，まずページフォルトが発生する．このとき，カーネルはページフォルトハンドラの動作として使用中の物理メモリの一部をスワップ領域に退避させる（この動作を **スワップアウト** と呼ぶ）．どの領域をスワップアウトするかは，近いうちに使わなさそうな領域を所定のアルゴリズムによって決定する（アルゴリズムの詳細にはここでは触れない）．退避したページのスワップ領域上の位置は，ページテーブルと同要領でスワップ領域管理用の表に記載される．

再び空きメモリが十分になったとき，カーネルはスワップ領域に退避させていたデータを物理メモリに戻す（この動作を **スワップイン** と呼ぶ）．

継続的にメモリ不足の場合はメモリアクセスのたびにスワップインとスワップアウトが繰り返される **スラッシング** という状態になる．この状態に陥るとシステムは大抵殆ど応答しなくなり，ハングアップしたり（真に）OOMが発生する．


## 階層型ページテーブル

ページテーブルは，実際には配列のようなフラットな構造ではなく階層化されている．ページテーブルエントリのサイズは8バイトであり，もしページテーブルがフラットだとしたら，例えばx86\_64の1プロセスあたり128TB程度の仮想アドレス空間を扱うには 8バイト × 128TB / 4KB = 256GB の巨大なメモリが1プロセスだけで必要となり，そんなはずはない．

実際のx86\_64のページテーブルの構造はもっと複雑だが，ここではより簡単なモデルとして見る．階層型ページテーブルは区間で切られたn分木の構造をしており，物理アドレスに割り当たっている領域を含む区間はより細かい区間に分かれた下位のページテーブルを再帰的に保持する構造をとる．物理アドレスが割り当てられた仮想アドレスがその区間にひとつもない場合は下位のページテーブルが保持されない仕組みになっており，それゆえに（空間に対して実際に使っている範囲が疎であるような典型的状況では）アドレス空間の広さに対して実際に占有するデータは小さく抑えられている．

実際に使用しているページテーブルのメモリ量は `sar -r ALL` の出力の `kbpgtbl` フィールドで見ることができる．


## ヒュージページ

前節で見たように，プロセスのメモリ使用量が増えるとページテーブルもそれに伴って大きくなる．それゆえ，メモリ使用量の大きいプロセスは `fork()` にも時間がかかるようになる（親プロセスが使用している “本来のメモリの内容” 自体はコピーしないが，ページテーブルのコピーは発生するため）．これを解決する仕組みとして **ヒュージページ** という仕組みがある．

ヒュージページはその単純な名前の通り巨大なページテーブルで，階層型ページテーブルの密になった部分の記録方法を “大きい区間を大きい区間にそのままマップする” という方式にすることでエントリの個数を節約するもの．

Linuxにはさらに **トランスペアレントヒュージページ** という仕組みがあり，これは仮想アドレス空間内の連続する複数のページが所定の条件を満たした時に自動でそれらをまとめてヒュージページ化してくれるというもの．ただし，条件を満たしたとき・満たさなくなったときにヒュージページの組み立てと分解が発生して局所的にオーバーヘッドがかかるというデメリットもあり，システムによっては構築時にトランスペアレントヒュージページを無効に設定することもある．


# 6章 記憶階層

コンピュータの記憶装置には

- レジスタ
- キャッシュメモリ
- メモリ
- ストレージデバイス

という階層があり，手前ほどサイズが小さく，容量あたりの価格が高く，アクセス速度が速い．この各階層について，どの程度サイズや性能に差があるのか，これらの差を勘案してハードウェアやLinuxはどのような仕組みを実現しているのかにこの章で触れる．


## キャッシュメモリ

一般に計算機の動作は以下のような過程を経る：

1. 命令をもとにメモリからレジスタにデータを読み出す．
   - 本当はこの命令自体の読み出しもメモリから行なわれる．
2. レジスタに入ったデータをもとに計算を行なう．
3. 計算結果をメモリに書き戻す．

近年のハードウェアはレジスタ上での計算（1ナノ秒未満）に比べメモリアクセスの所要時間（数十ナノ秒）はずっと長い．したがってどれだけ計算はメモリアクセスが律速となる．これを緩和する機構が **キャッシュメモリ**．

キャッシュメモリからレジスタへの読み出しにかかる時間はメモリのそれに比べて数倍から数十倍高速であり，これを利用して1と3のフェーズを高速化できる．

キャッシュメモリは通常CPUに内蔵されるが，外部についているものもある．

キャッシュメモリの動作は，カーネルは関与せずにハードウェア内で完結している．

メモリからレジスタにデータを読み出す際に，キャッシュメモリにも同じデータとそのアドレスを読み出しておく．このあとCPUが同じアドレスのデータを読み出す際にはメモリではなくキャッシュメモリを見ればよい．

キャッシュメモリの要素を **キャッシュライン** と呼ぶ．読み出されるデータの大きさはCPUごとに決まっており，**キャッシュラインサイズ** と呼ばれる．

一方，書き込みの場合もやはりキャッシュメモリに対応するアドレスが読み出されている場合はその内容を書き換える．そしてそのキャッシュラインには「メモリから読み出された後に書き換えが行なわれた」という印をつけておく．この印がついているキャッシュラインは **ダーティ** であるという．呼ぼれているキャッシュラインは，書き込みを行なった以降の所定のタイミングでバックグラウンド処理としてメモリに書き戻され，印が取り除かれる（これを **ライトバック方式** と呼ぶ）．これにより書き込み単体はキャッシュメモリへのアクセスだけの所要時間で済む．


## キャッシュメモリがいっぱいになったら

キャッシュメモリの全てのキャッシュラインが埋まった場合，キャッシュされていないアドレスに対して読み書きを行なうと，既存のキャッシュラインのうちの1つが破棄され，そこに新しい内容が読み出される．破棄することにしたキャッシュラインがダーティな場合は，その内容をメモリに書き戻してから破棄する．ダーティなキャッシュラインを破棄するのは比較的望ましいことではなく，特に全てのキャッシュラインが継続的にダーティな状況が続くとやはりスラッシングが発生し，キャッシュメモリの性能は大幅に低下する．


## 階層型キャッシュメモリ

x86\_64ではキャッシュメモリも階層型の構造をとり，各階層によってサイズ，レイテンシ，どの論理CPUで共有するかなどが異なっている．各階層は **L1**，**L2**，**L3** などと呼ばれる（“L” は “level” の頭文字）．どのレベルまであるかはCPUに依存する．最もレジスタ寄りでサイズが小さく高速なのがL1キャッシュで，レベルが大きいほどレジスタから遠く，サイズが大きく，低速になる．

実際のマシンでは，キャッシュメモリの情報は `/sys/devices/system/cpu/cpu〈N〉/cache/index〈M〉/` といった形のディレクトリにあるファイルの中身を見ると確かめることができる．


## キャッシュの実験

プロセスがアクセスするデータのサイズによってアクセスにかかる時間がどのように変化するかを実験するとキャッシュメモリが機能している様子が確かめられる．以下のような挙動をするプログラムを書いて走らせる：

1. 第1引数に指定されたキロバイト数のメモリを確保する．
2. 確保したメモリ領域内に所定の回数だけシーケンシャルにアクセスする．
3. 1アクセスあたりの所要時間を表示する．

コンパイルは `-O3` オプションをつけて行なう．

結果を見ると，階段状に跳ね上がる箇所があり，たしかにキャッシュメモリに収まる量は一度に取得してキャッシュメモリに対するアクセスが行なわれていそうなことが確かめられる．


## 参照の局所性

実験用につくったプログラムに限らず実際的なプログラムでもキャッシュメモリは性能を向上させることが多い．というのも，プログラムは典型的には以下のような性質をもつため：

- **時間的局所性**： 或る時点でアクセスされたデータは，近い将来に再びアクセスされやすい．
  + 特にループ処理のコード領域など．
- **空間的局所性**： 或る時点で或るアドレスのデータにアクセスされると，近い将来にそれに近いアドレスのデータにアクセスすることが多い．
  + 特に配列の走査など．


## まとめ

データ配置やアルゴリズムを工夫して単位時間あたりのメモリアクセス範囲を狭くすることでキャッシュメモリの効果を活かし高速な動作を実現できる．一方で，システムの変更などによってプログラムの性能が大幅に落ちる場合，局所的なメモリアクセス範囲がキャッシュメモリに乗り切らなくなったことが疑える．


## Translation Lookaside Buffer

ここまでに述べたキャッシュの機構は物理アドレスに関するもの．

ところでプロセスが所定の仮想アドレスのデータにアクセスするには以下の手順を踏む必要があった：

1. 物理メモリ上にあるページテーブルを参照し，仮想アドレスを物理アドレスに変換する
2. 物理アドレスを用いてアクセスする

ということは，キャッシュ機構によって性能が向上できるのは2の段階だけで，1の段階は（ページテーブルがキャッシュメモリではなくメモリ上にあることから）高速化できていない．これではキャッシュメモリの利点があまり行かせていないことになる．

この問題の解決のために **translation lookaside buffer**（**TLB**）という機構がある．これは簡単にいえば “ページテーブル用のキャッシュメモリ”．詳細は触れないが，存在は知っておくとよい．


## ページキャッシュ

CPUからメモリへのアクセス速度に対し，ストレージへのアクセス速度はさらに桁違いに遅い．この差を埋めるのに使われるのが **ページキャッシュ** という仕組み．

ページキャッシュはやはりキャッシュメモリとよく似た機構で，すなわちページ単位でストレージの内容をキャッシュしておく仕組み．プロセスがファイルのデータを読み出したいと言った場合，カーネルはプロセスのメモリにファイルのデータを直接コピーするのではなく，一旦カーネルのメモリ上にあるページキャッシュの領域にコピーしてからそのデータをプロセスのメモリにコピーする．カーネルはページキャッシュにどのファイルのどの範囲を読み出したかをカーネルのメモリ内に保持する管理領域を持っている．以後，該当ファイルの該当範囲内を読み出すとき，カーネルはページキャッシュからデータを読み出す．これによりファイルへのアクセスが実際にストレージに読みに行くよりもずっと高速に終わる．ページキャッシュは全プロセスの共有資源であり，どのプロセスからの読み出しにも対処できる．

プロセスからの書き込みの要請に対しても，カーネルはやはりページキャッシュにのみデータを書き込む．そして実際のストレージにまだ書き戻されていないことを表す印をつける．この印がついたページキャッシュ内のページのことを **ダーティページ** と呼ぶ．ダーティページの内容は，やはり所定のタイミングでバックグラウンド処理として実際のストレージへと書き戻され，ダーティページであるという印もそのとき消される．

ページキャッシュは，メモリ上に空きがある限り，新たなファイルの範囲にアクセスが生じるたびに増えていき，メモリが足りなくなってきたときにカーネルによって解放される．このときはまずダーティでないページを解放し，それでも足りないならダーティページを書き戻した上で解放する．ダーティページを破棄しなければならない場合はストレージへのアクセスが発生するので性能が悪化しがちである．このダーティページの書き戻し多発によりシステムがハングアップする例はかなり多いらしい．


## 同期書き込み

ダーティページの内容はまだストレージに書き込まれていない揮発性の内容なので，書き戻しよりも前に電源遮断が発生したりすると失われる．このようなリスクを許容できない場合は，そのような書き込みが失われてはいけないファイルを `open()` システムコールで開くときに `O_SYNC` フラグという特別な設定を加える．これにより，`write()` システムコールを発行するごとに，ページキャッシュへの書き込みだけでなく実際のストレージへの書き戻しも同期的に行なわれる．


## バッファキャッシュ

ページキャッシュと似た仕組みとして **バッファキャッシュ** というものがあり，これはファイルシステムを介さずに後述の **デバイスファイル** という機構を用いてストレージデバイスに直接アクセスする際に使う（7章参照）．基本的にはページキャッシュとこのバッファキャッシュがストレージ内のデータをメモリ上に置いておく機構と考えてよい．


## ファイル読み出しの実験

ページキャッシュが機能していることを確かめるには，同じ（新しい）ファイルに2回アクセスしてそれぞれの所要時間を測ったり，`free` コマンドや `sar -r` を使ってファイル読み出し前後でのページキャッシュ使用量を確認したりするとよい．

著者の環境の実験では，1GB程度のファイルを作成して読み出すと，1回目は約2秒かかり，その直後実際にページキャッシュが1GB程度増えていた，そして2回目の読み出しを行なうと約0.1秒で終了した．

また，実際にファイル作成，読み出し1回目，読み出し2回目をシェルスクリプトで動作させながら `sar -B` をバックグラウンドで動かして時系列を見る．すると，最初のファイル作成時に合計1GBのページアウトが発生し，読み出し1回目でページインが発生，読み出し2回目では（システム上の他の処理で生じるわずかな変化を除いて）ページインは発生しない．ストレージデバイスのI/Oも `sar -d -p` で見ることができ，これをみるとたしかにファイル作成時に書き込みが，読み出し1回目のみで読み出しが発生していることがわかる．


## ファイル書き込みの実験

書き込みも同様にページキャッシュにより性能が上がり，また書き込みの間ページアウトが発生しないことも確認できる．


## チューニングパラメータ

Linuxにはページキャッシュを制御するためのパラメータがあり，`sysctl` で設定できる．以下がその一部：

- `vm.dirty_writeback_centisecs`： ダーティページの書き戻しの周期．デフォルトでは500センチ秒，すなわち5秒．
- `vm.dirty_background_ratio`： 全物理メモリに対してダーティページがこれに指定された割合（パーセント単位）を超えた場合は書き戻しを行なう．
- `vm.dirty_background_bytes`： 上の項目のバイト単位版．0は未指定扱い．
- `vm.dirty_ratio`： ダーティページの割合がこれに指定された割合を超えた場合はプロセスによるファイル書き込みの際に同期的に書き戻しを行なう．
- `vm.dirty_bytes`： 上の項目のバイト単位版．0は未指定扱い．


## ハイパースレッド

CPUによる計算時間よりもメモリアクセスの方がずっと遅いため，一般にプロセスの動作はメモリによって律速されており，CPUは時間軸上結構高い割合で待機している．ちなみにメモリアクセスのほかにもCPU資源を空費してしまう理由はいくつもある．

待ち時間で空費されがちなCPU資源を有効活用するのが **ハイパースレッド機能**．これは，1個の物理的なCPUに対してレジスタなどの資源を複数（大抵は2つずつ）用意して，システムからは複数の論理CPUとして認識されるようにしたもの．このときの論理CPUを **ハイパースレッド** と呼ぶ．要するに “片側が待ち時間の場合は他方のCPUとして使う” という仕組み．排他制御を要する以上，2つのハイパースレッドに分割すれば2倍くらい良くなるかというと別にそうでもなく，現実的には2〜3割ほど向上すれば良い方．利用することでむしろ遅くなることもある．

手元の環境でどの論理CPUの組が物理的なCPUを共有しているかは `/sys/devices/system/cpu/cpu〈CPU番号〉/topology/thread_siblings_list` を見るとわかる．


# 7章 ファイルシステム

## Linuxのファイルシステム

Linuxがサポートするファイルシステムの実装は **ext4**，**XFS**，**Btrfs** などいろいろあるが，いずれもユーザからはシステムコールの発行という共通のインターフェイスによって隠蔽されている：

- 作成，削除： `creat()`，`unlink()`
- 開く，閉じる： `open()`，`close()`
- 開いたファイルからの読み出し： `read()`
- 開いたファイルへの書き込み： `write()`
- 開いたファイルを所定の位置に移動： `lseek()`
- ファイルシステム依存の特殊処理： `ioctl()`

これらのシステムコールが発行されると，以下のような手順でデータが読み書きされる：

1. カーネル内の全ファイルシステムに共通の処理が動作し，操作対象のファイルシステムが判別される．
2. 各ファイルシステムを扱う処理を呼び出して，システムコールごとの処理を行なう．
3. データの読み書きをするなら，デバイスドライバに処理を依頼する．
4. デバイスドライバがデータを読み書きする．


## データとメタデータ

ファイルシステムで操作する対象には，ユーザがファイルの内容などとして作成したデータのほかに，ファイルの名前やストレージデバイス上の位置やサイズ，作成・最終更新時刻，権限などの補助的な情報であるメタデータがある．

**`df` コマンド** によってファイルシステムのストレージ使用量を見ることができるが，これは（データに比べると小さいが）メタデータのサイズも含んでいる．特に小さいファイルを大量につくったりするとメタデータの容量に占める割合が高かったりする．


## 容量制限

ファイルシステムの容量が足りなくなると（とりわけroot権限で動作するプロセスが使用する容量が確保できなくなって）システムが正常に動作しなくなるおそれがあるので，**クォータ** (quota) と呼ばれる，使用できるファイルシステムの容量に用途ごとの制限を設ける機能がある．クォータには以下の種類がある：

- **ユーザクォータ**： ファイルの所有者となるユーザごとに容量を制限するもの．例えば特定のユーザによって `/home` が大きな容量を占めてしまうことを防ぐ．
- **ディレクトリクォータ**（**プロジェクトクォータ**）： 特定のディレクトリの容量を制限する．ext4とXFSではこれが使える．
- **サブボリュームクォータ**： ファイルシステム内の **サブボリューム** という単位ごとに容量を制限する．おおよそディレクトリクォータに似る．Btrfsではこれが使える．


## ファイルシステムの不整合

システムを運用しているとファイルシステムに不整合が生じることがある．典型的にはストレージの読み書き中に電源が遮断されて停止した後に復活した場合．例えば，ディレクトリの移動はリンクの張り替え処理だが，移動途中の「移動後の箇所からリンクを張ったが，移動前の箇所からのリンクはこれから消す」という状態で電源が遮断されると，移動前の箇所からも移動後の新しい箇所からも移動対象のディレクトリにリンクを張っている不整合の生じた状態で復帰してしまう．このような場合，遅かれ早かれファイルシステムがその異常を検知し，最悪の場合はシステムがパニックしてしまう．

ファイルシステムの不整合を防ぐ技術は色々あるが，とくに **ジャーナリング** と **コピーオンライト** がよく知られる．ext4とXFSはジャーナリング，Btrfsはコピーオンライトを採っている．


## ジャーナリング

この方式では，ファイルシステム内に **ジャーナル領域** という（ユーザからは見えない）特別な領域を用意し，ファイルシステムの更新の際には以下のような手順をとる：

1. 更新に必要な原子的な処理の一覧である **ジャーナルログ** をジャーナル領域に書き出す．
2. ジャーナル領域の内容に基づいて，実際にファイルシステムの内容を更新する．

例えば `path/to/bar/` を `path/to/foo/bar` に移動する場合は以下のようになる：

1. ジャーナルログに以下を書き出す：
   - (A) `path/to/foo/` から `bar/` へのリンクを張る．
   - (B) `path/to/` から `bar/` へのリンクを消す．
2.
   1. (A) を実行する．
   2. (B) を実行する．

1と2の間で電源が失われた場合も，2-1と2-2の間で電源が失われた場合も，それぞれジャーナル領域に書かれた動作を最初から再開すればよい（裏を返して言えば，これを可能とするために，原子的な処理の記述は冪等性をもつ必要がある）．ジャーナルログの書き出しに失敗するタイミングで電源が喪失した場合は再開後何もしない．こうして，どのタイミングで電源が喪失しても処理前か処理後かのいずれかの整合した状態に復帰できる．


## コピーオンライト

この方式をとるファイルシステムでは，一度作成されたファイルも一般には更新のたびに配置場所が変わり，所属するディレクトリからのリンクもつけ替えられる．この場合，更新途中で電源が遮断されても，復帰時につくりかけの方を捨てれば不整合は生じない．

- **疑問： おそらくつくりかけの方には「これはつくりかけです」というマークをつけておいて識別するのだと思うが，このマークを消すタイミングと古い方のファイルを消すタイミングには差異がありそうで，どちらが先だとしてもその間で電源が遮断されたら整合した状態に復帰できないんじゃなかろうか？**


## ファイルシステムの不整合への対策

仕組みとしては不整合が生じないようになってはいるものの，ファイルシステムの実装上の不具合で不整合が生じることは依然として少数ながらある．その場合どうするか？

最も一般的なのは定期的にバックアップをとっておいて不整合が生じたらそのバックアップの時点に復帰するもの．

バックアップを取っていないなどの事情があれば，ファイルシステムに備わっている復旧用コマンドが使えるかもしれない．ただし，これは以下のような点で推奨できない：

- ファイルシステムを全走査するので，かなり時間を要する．数TBに対して数時間〜数日程度．
- 時間がかかるわりに失敗に終わることも多い．
- ユーザが望んだ状態に復帰できるとは限らない．不整合に関わるデータやメタデータが容赦なく削除されたりする．例えば（移動途中の不整合のように）リンクが重複している箇所があったら，その指す先のファイルがまるごと削除されてしまうことはありうる．


## ファイルの種類

Linuxで扱うファイルは通常のファイルとディレクトリの他に **デバイスファイル** という種類がある．Linuxは自分が動作しているハードウェア上のデバイスをほぼすべてこのデバイスファイルによってファイルとして表現している（ネットワークアダプタは例外的に対応するファイルなし）．`/dev` 以下にあるのがそれ．

デバイスファイルに対しても通常のファイルと同様に `open()` とか `read()` といったシステムコールを発行したりできる．デバイス固有の操作については `ioctl()` を使う．デバイスにアクセスできるのは通常rootのみ．

Linuxはファイルとしてアクセスできるデバイスを **キャラクタデバイス** と **ブロックデバイス** に分類している．

`ls -l` の結果で先頭が `d` になっているものはディレクトリだが，同じ要領で `c` がキャラクタデバイス，`d` がブロックデバイス．実際に `ls -l /dev/` を叩くと見られる．例えば `/dev/tty` はキャラクタデバイス，`/dev/sda` はブロックデバイス．


## キャラクタデバイス

キャラクタデバイスは，読み出しと書き込みはできるがランダムアクセスはできないという特徴をもつ．以下がキャラクタデバイスの例：

- ターミナル
- キーボード
- マウス

実際，ターミナルは `read()` や `write()` で制御できる．Bashを起動し，`ps ax | grep bash` でBashプロセスに対応づいているターミナルを見つける（2列目がターミナル）．ここでは `/dev/pts/9` だったとする．ここで

```
$ sudo su
# echo hello > /dev/pts/9
```

とすると，実際に同じBashのウィンドウ上に `hello` が追記される．ちなみにこれは通常の `$ echo hello` と同じだが，この場合は `echo` が標準出力に書き込んでいて，かつLinuxが標準出力を端末と紐づけているため．

ターミナルを2つ開いていて相方のキャラクタデバイスに上記の方法で書き込むと実際に相方側に文字列が追記されるのを確認できる．


## ブロックデバイス

ブロックデバイスは読み書きに加えて（通常のファイルのように）ランダムアクセスが可能．前述の通りブロックデバイスは通常ファイルシステムを介してアクセスするが，以下の状況のように直接操作する場合もある：

- **パーティションテーブル** の更新（**`parted` コマンド** などを用いる）
- ブロックデバイスの水準でのデータのバックアップとリストア
- ファイルシステムの作成（各ファイルシステムの `mkfs` 相当のコマンドを用いる）
- ファイルシステムのマウント（**`mount` コマンド** を用いる）
- `fsck` による整合性チェック

ブロックデバイスに対しても（ファイルシステムの抽象化を経由せずに）ファイルシステムのエンコード結果の生のバイト列を通常のファイルとして読み書きすることができる．ただし当然ながら危険な行為なので実験用のファイルシステムに対する操作のみにとどめておくべし．


## さまざまなファイルシステム

ストレージデバイス上に構築されたものに限らず，他にも色々な形態のファイルシステムがある．


## メモリベースのファイルシステム

メモリ上に構築される **tmpfs** というファイルシステムがある．揮発性だが高速にアクセスできる．


## ネットワークファイルシステム

ネットワークを介して繋がっているリモート上のファイルにアクセスするためのファイルシステムを **ネットワークファイルシステム** と呼ぶ．リモートがWindowsの場合は **cifs**，LinuxなどUNIX系OSの場合は **cfs** など．


## 仮想ファイルシステム

カーネル内の種々の情報を取得したり変更したりするために色々なファイルシステムが存在する．


### **procfs**

システム内に存在するプロセスについての情報を保持している．通常は `/proc` 以下にマウントされる．`/proc/〈PID〉/` 以下に各プロセスに紐づく情報がある．

+ `/proc/〈PID〉/maps`： プロセスのメモリマップ
+ `/proc/〈PID〉/cmdline`： プロセスのコマンドライン引数
+ `/proc/〈PID〉/stat`： プロセスの状態や，これまでに使用したCPU時間など

プロセス別のほかにも以下がある：

+ `/proc/cpuinfo`： システムが搭載するCPUに関する情報
+ `/proc/diskstat`： システムが搭載するストレージデバイスに関する情報
+ `/proc/meminfo`： システムが搭載するメモリに関する情報
+ `/proc/sys/*`： カーネルの各種チューニングパラメータで，`sysctl` や `/etc/sysctl.conf` でいじれるパラメータと対応している


### **sysfs**

`procfs` の濫用を防ぐために新設された．通常は `/sysfs` 以下にマウントされる．

+ `/sys/fs`： システムに存在する各種ファイルシステムの情報


### **cgroupfs**

1個以上のプロセスからなる集合に対してリソースの制限を設ける **cgroup** という機能を操作するためのファイルシステム．rootのみがいじれる．通常は `/sys/fs/cgroup` 以下にマウントされる．

cgroupで設定可能なのは，例えばそのプロセスの集合がCPUの全リソースのうち50%までしか使ってはいけないとか，物理メモリを1GBまでしか使ってはいけないといった制限．これはDockerなどのコンテナ管理ソフトウェアで個々のコンテナのリソースの制限にも使われている．クラウドサービスなど，1つのシステム上で複数の顧客のコンテナや仮想マシンが動作しているような状況でも使われる．


## Btrfs

Btrfsは比較的新しいファイルシステムで，（LinuxのもととなったUNIXが制作された頃からある）ext4やXFSよりもリッチな機能を提供している．


### マルチボリューム

XFS（やext4）は1つのパーティションに対して1つのファイルシステムを作成するのが基本で，下に **LVM** (**logical volume manager**) という機構が挟まって複数のパーティションを **ボリュームグループ** というものにまとめてから **論理ボリューム** という単位に切り出すように抽象化し各々の上にXFSのファイルシステムを載せたりしたが，Btrfsは **マルチボリューム** といって最初から複数のストレージデバイスやパーティションを **ストレージプール** というものにまとめてその上で **サブボリューム** という単位に切り出す機構が備わっている．その意味ではBtrfsはファイルシステムというよりは従来のファイルシステム＋LVMである．

ストレージプールに対してはストレージデバイスを事後的に追加したり除去したりすることが可能．稼働中でも可能．


### スナップショット

Btrfsはサブボリューム単位でスナップショットを採取することができる．これはバックアップのようにいわゆるdeep copyを行なうのではなく，メタデータを残しておくshallow copyによって実現されている（コピーオンライト方式であるおかげでこれが実現できる）．


### RAID

Btrfsではファイルシステムの水準でRAID構成が組める．以下をサポートしている：

- **RAID0**： 単にデータを冗長性なく複数台に分散させたもの．読み書きが高速になるが，冗長性はないのでストレージデバイスのどれかが故障するとそのストレージデバイスに記録されていたものは失われる．
- **RAID1**： 同じ内容を複数台に記録したもの．**ミラーリング** ともいう．書き込みに台数分の時間的コストがかかるが全台故障しないかぎり記録が失われることはない．
- **RAID10** (**RAID1+0**)： RAID1とRAID0の組み合わせ．つまりまずRAID1で分散し，その各々がRAID0で分散したもの．
- **RAID5**： データをRAID0のように最低3台に分散させつつ，障害に備えて復元用のパリティも保持しておくもの．1台壊れても復旧できる．2台以上だと復旧できないデータが出る．
- **RAID6**： RAID5のパリティを2種類にしたもの．2台の故障までは復旧できる．3台以上だと復旧できない．
- **dup**： 同じデータを単一のストレージデバイス内で二重化するもの．


### データ破損検知/修復

Btrfsは（該当のRAID構成なら）ストレージ内の一部が破損した場合に修復する機能があり，例えば書き込み時に稀に発生するビット化けなども検出できたりする．

検知可能なのは，データ・メタデータ共にチェックサムの機構をもっていることによる．読み出しの際にチェックサムエラーが起きると，Btrfsは読み出したプログラムに対してI/Oエラーを通知する．このときRAID0でなければ，別に取っていたデータでチェックサムが一致した場合，そちらからコピーして修復する．

ext4やXFSも破損の検知はできるが，修復可能なのはBtfrsのみ．


# 8章 ストレージデバイス

この章ではストレージデバイスとそれに関するカーネルの機能を扱う．ストレージデバイスの代表としてHDDの構造に触れ，その特性を活かして高いI/O性能を出すためのカーネルの機構である **ブロックデバイス層** について記す．


## HDDのデータ読み書きのしくみ

HDDはデータを磁気情報で表現してそれを **プラッタ** という円盤状の磁気ディスクに記憶する．データは **セクタ** という単位で読み書きされる．プラッタは半径方向と円周方向に極座標的にセクタに区切られており，**スイングアーム** という部品（イメージとしてはレコードの針のような具合にプラッタについていて自由度1で動く）の先頭についている **磁気ヘッド** により読み書きが行なわれる．基本的には，円周方向の座標はプラッタを回転させることによって，半径方向の座標はスイングアームをずらすことによってアクセスする．以下のような流れでHDDからシステムへのデータの転送が行なわれる．

1. デバイスドライバがデータの読み書きに必要な情報をHDDに渡す．セクタ番号，セクタの数，アクセスの種類（読むのか書くのか）など．
2. プラッタの回転とスイングアームの移動により所望のセクタに磁気ヘッドの位置を合わせる．
3. データの読み書きを行なう．
4. 読み出しの場合はこれで完了．

1と4は電気的に行なわれる高速な処理だが，2と3は機械的に行なわれる遥かに遅い処理で，HDDに対する操作の所要時間の大部分は機械的処理によって生じる．


## HDDの性能特性

HDDは複数の連続するセクタを1度のアクセスによって取り出せる．ただしHDDごとに一度に読める量の上限はある．上限を超えるようなアクセスは（システムコールとしては1度の発行でも）ブロックデバイスの水準で複数のHDDアクセスに分割されている．

こうした性能特性を勘案し，各種ファイルシステムは（特にHDDを使う場合は）各ファイルのデータをなるべく連続した領域に配置するようにしている．


## HDDの実験

未使用のパーティションを使い，（ファイルシステムの抽象化を経ない）生のデータを読み書きして以下を見てみる（必ず未使用のパーティションを使うこと．そうでないと記録されているデータが破壊される）：

- I/Oサイズによる性能の変化
- シーケンシャルアクセスとランダムアクセスの違い


## 実験プログラム

次のような引数列を受け取って動作するプログラム `./io` を書く：

1. ファイル名
2. カーネルによるI/O支援機構（後述）を有効にするかどうか．`on | off`
3. 読み出しか書き込みか．`r | w`
4. アクセスパターン．シーケンシャルかランダムか．`seq | rand`
5. 1回あたりのI/Oサイズ（KB単位）


## シーケンシャルアクセス

`./io /dev/〈実験用に用意したHDD〉 off r seq $i` を `i = 4, 8, 16, 32, …` に対して実行すると，基本的には大きいほどスループット性能が上がることが確かめられる．しかし（ここでは）1MBほどで頭打ちになりそれ以上は向上しないため，HDDの1度にアクセスできる量の上限が1MB程度であることがわかる．`r` を `w` に変えて書き込みに対して実験してもよく似た結果．


## ランダムアクセス

`seq` を `rand` にして行なうと，`$i` が小さいうちは殆どスループットが上がらない．大きくなるとプログラム全体に於けるアクセス待ちの時間の割合が減少するためにスループットが向上するが，シーケンシャルアクセスほど良くはない．


## ブロックデバイス層

7章でも触れたが，LinuxではHDDやSSDといった一定のデータ量（HDDやSSDの場合はセクタ）を単位としてランダムアクセスできるデバイスをブロックデバイスと呼んでおり，このブロックデバイスはデバイスファイルという特殊なファイルにより直接アクセスするか，ファイルシステムの抽象化を介してアクセスする（ほとんどのソフトウェアは後者の手段をとる）．

ブロックデバイスには共通の処理が多く，そのような処理は各デバイスドライバが実装するのではなく，カーネル内の **ブロックデバイス層** という部分によって共通化されている．ブロックデバイス層の機能のうち特に **I/Oスケジューラ** と **先読み** について以下で触れる．


## I/Oスケジューラ

これはブロックデバイス層へのアクセスを一定期間溜めておき，以下のような加工をしてから実際にデバイスドライバにI/Oを要求することによって性能を向上することをねらったもの：

- マージ： 複数の連続するセクタへのI/O要求を1つにまとめる
- ソート： 複数の不連続なセクタへのI/O要求を番号順に並べ替えて行なう

特にソート後にマージできる場合などもあり，その場合は相乗効果で高い性能が期待できる．このI/Oスケジューラにより，ユーザプログラムの作成者がプロックデバイスの性能特性を意識していなくても或る程度は性能が出てくれるようになっている．

ただし，読み出しのマージ処理に関しては，I/Oスケジューラの出番があるのは，複数のプロセスが並列に読み出しをする場合や，非同期I/Oという書き込みの完了を待たずにプログラムが動く状況の場合（ここでは詳しくは触れない）．


## 先読み

プログラムからデータへのアクセスは（6章で述べたように）空間的局所性があり，先読みはこれを利用した機能で，ストレージデバイスの或る領域にアクセスした場合にその直後に続く領域も陽にI/O要求がなくとも（I/O要求がくる可能性が高いと推測して）事前に読んでおくというもの．後に実際にその領域に対するI/O要求が来た場合はストレージデバイスにアクセスせずに先読みした部分を返せる．I/O要求が来なかった場合は単に先読みしたデータを捨てる．


## 実験

### シーケンシャルアクセス

前の実験で出てきたプログラムの第2引数を `on` に変えて同様の実験をやってみる．その結果，読み出し・書き込み共にI/Oサイズが比較的小さい場合（2^6KB以下）もスループットが向上することが確かめられた．

`iostat` で実際にI/Oが起きているタイミングを見ると，先読みをしていることがわかる．マージ処理は `rrqm/s` というフィールドに表示されるが，（同期I/Oなので）機能していない．

一方で書き込みの高速化はマージ処理が性能を向上させており，`iostat` の結果の `wrqm/s` のフィールドに現れている．


### ランダムアクセス

第4引数を `seq` ではなく `rand` にして同様に実験すると，まず，支援機構があってもスループットはそれほど大きくならないことがわかる．支援機構なしのランダムアクセスと比較しても，読み出しはほぼ変わらない．一方書き込みはたまたま発生したマージによって若干改善するらしい．

- **疑問： ここの図08-22の読み方がよくわからない．**


## SSDのしくみ

SSDはデータへのアクセスに関してHDDと違って機械的な動作がなく，電気的な操作だけで済むようにつくられたストレージデバイス．このためアクセスが高速だが，一方で容量あたりの価格はSSDの方が高いため，当面はHDDとSSDが両方共存して使われるだろう．


### 実験

SSDも同様に実験する．`off`・`seq`（I/O支援機能無効，シーケンシャルアクセス）を走らせると，I/Oサイズが大きいほどスループットが高いのはHDDと同様だが，読み出し・書き込み共にHHDより数倍は速いことがわかる．

SSDに対して `off`・`rand`（I/O支援機構無効，ランダムアクセス）で実験すると，やはりI/Oサイズが大きいほどスループットは向上する．これもSSDとHDDで比較すると，特にI/Oサイズが小さいときはSSDの方が歴然と速い（1桁か2桁は違う）．また，SSD自身のシーケンシャルアクセスの場合と比較しても読み出しは2^8KB以上になると殆ど性能差がないことがわかる．書き込みは読み出しに比べると差異がある．

`on`・`seq`（I/O支援機構有効，シーケンシャルアクセス）の場合をSSDとHDDで比べると，これも数倍程度違いが出る．また，SSD自身の支援機構が無効な場合と比較すると，（HDDでみたのと同様に）一定のサイズになると上限にあたり性能が限界に達するらしいこともわかる．特に書き込みはI/O支援機構が有効な場合の方が性能が悪い．これはI/O要求を溜めておくオーバーヘッドが無視できなくなってくるためと思われる（HDDだと機械的処理の方がはるかに時間がかかるので現れなかった）．
