
# 2章 ユーザモードで実現する機能

ユーザモードからカーネルモードの処理が必要なときは **システムコール** を介して呼ばれる．

## システムコール

システムコールは以下のような操作を担う：

- プロセス生成/削除
- メモリ確保/解放
- プロセス間通信
- ネットワーク処理
- ファイルシステム操作
- ファイル操作（デバイスへのアクセス）


### CPUのモード遷移

システムコールはCPUの特殊な命令を実行することで発行する．システムコールが発行されると割り込みが発生し，ユーザモードからカーネルモードに遷移して該当する処理が走り，その処理が終わると再びユーザモードに戻る．カーネルモードの開始時にはプロセスからの要求が正当であるかがチェックされ，不正なら失敗させる．（もしプロセス自身がモードを切り替える手段を持っているなら権利の設計としておかしいため，このような仕組みになっている）．


### システムコール呼び出しの様子

`strace -o 〈Log〉 〈Command〉`  で，通常のコマンド `〈Command〉` による処理に対して発行されたシステムコールのログが `〈ログファイル〉` に出力される．例えばHello Worldプログラムでは `write()` が発行されている様子がわかる（ほかにも前後にプロセスの開始処理と終了処理に伴うシステムコールがたくさん発行される）．


### 実験

**`sar` コマンド** で各CPUコアがユーザモードとカーネルモードのどちらで実行しているかの割合を見ることができる．


### システムコールの所要時間

`strace` の `-T` オプションを使うと各システムコールの処理に何マイクロ秒かかっているのかを出力できる．


## システムコールのラッパー関数

システムコールは（CPUの実装に依存することから）アーキテクチャ依存のアセンブリコードを使って呼び出す必要があり，C言語などの或る程度高級な言語からはこれをラップして “FFI的に” 呼び出せるようにしてある（OSがライブラリを提供している）．


## 標準Cライブラリ

C言語にはISOが定めた標準ライブラリがあり，Linuxもこれに準拠してライブラリを提供している．通常はGNUの `glibc` で，これがシステムコールのラッパーを含む．

プログラムがどんなライブラリをリンクしているかは **`ldd` コマンド** で確かめることができるので見てみるとよい．


## OSが提供するプログラム

OSはライブラリだけでなくプログラムも提供しており，（よく知っているように）以下のようなものがある：

- システムの初期化： `init`
- OSの挙動変更： `sysctl`，`nice`，`sync`
- ファイル操作： `touch`，`mkdir`
- テキスト処理： `grep`，`sort`，`uniq`
- 性能測定： `sar`，`iostat`
- コンパイラ： `gcc`
- インタプリタ： `python`，`ruby`
- シェル： `bash`
- ウィンドウシステム： X


# 3章 プロセス管理

カーネルによるプロセス生成/削除の機能について触れる．実際のLinuxに於けるプロセス生成/削除には **仮想記憶** という概念が関わっており，これは5章で触れる．ここでは仮想記憶がない単純な場合を扱う．


## 2段階のプロセス生成

プロセス生成には2つの異なる目的がある：

- 同じプログラムによる処理を複数走らせる
- 全く別のプログラムを走らせる

これらそれぞれの目的に対して  **`fork()`** と **`execve()`** という2つのC言語の関数があり，これらは内部的に `clone()` と `execve()` というシステムコールを発行する．


## `fork()` 関数

`fork()` の呼び出しは次のような流れでプロセスを生成する：

1. 子プロセス用のメモリ領域を確保し，親プロセスのメモリの内容をそこにコピーする．
2. 親プロセスと子プロセスとで違うコードを実行するように分岐する．これは `fork()` の戻り値が親と子で異なるものになることを利用する．


## `execve()` 関数

全く別のプログラムによるプロセスを生成する場合は以下の流れによる：

1. 実行ファイルを読み出してプロセスのメモリマップに必要な情報を得る．
2. 現在のプロセスのメモリを新しいプロセスのデータで上書きする．
3. 新しいプロセスでプログラムを最初の命令から実行する．

つまり，プロセスが増えるのではなく既存のプロセスが書き換わる．

Linuxの実行ファイルは **ELF** (**Executable Linkable Format**) という形式で記録されている．**`readelf` コマンド** を使うと実際の実行ファイルにどんなメタデータ，コード領域，データ領域が含まれているのかを見ることができる．

実際にプログラムが走っている際のメモリマップは，`cat /proc/〈PID〉/maps` で見ることができる．

全く新しい別プロセスを生成する場合は，まず `fork()` で子プロセスを生成し，その後に `execve()` を呼んでプログラムを変更するという手順をとる．例えば `bash` が `echo` を呼び出すときも，まず `bash` の子プロセスをつくり，それを `echo` にプログラムに書き換えて実行を開始する．


## 終了処理

プログラムの終了は `_exit()` 関数が使われる．これは内部的に `exit_group()` システムコールを発行する．Linuxはこれによりそのプロセスに割り当てられていたメモリを回収する．

ユーザランドでは通常はこの関数を直接呼び出さず，そのラッパ的関数である `exit()` を呼び出す．C言語製の実行ファイルは `main()` が戻り値を返した時も同様の処理が走るようにコンパイルされている．


# 4章 プロセススケジューラ

この章では複数のプロセスが “同時に走っている” ようにするために必要な仕組みについて実験する．

マルチコアCPUでは1つのコアが1つの **論理CPU** という単位でLinuxに認識される．なお，**ハイパースレッド機能** が有効な場合は各コア内のハイパースレッドが論理CPUとして認識される（6章参照）．


## 実験プログラムの仕様

ひたすらCPU時間を使う処理をして有限時間で終わるプロセスを1個以上同時に動かして，全てが完了するまで以下を観察する：

- 或る時点で各論理CPU上でどのプロセスが動作しているか
- それぞれの進捗はどれだけか

実験プログラムは以下の引数をとる：

- `n`： 同時に動かすプロセス数
- `total`： プログラムを動作させる合計時間
- `resol`： 統計情報の採取間隔

（実際の実装が掲載されている）


## 実験

`n={1,2,4}`，`total=100`，`resol=1` で実験して比較する．結果は元の本参照（1論理CPU内では普通に並行していることが確かめられる）．


## 考察

以下のことがわかる：

- 1つの論理CPU上で各瞬間に動作できるプロセスは1つだけ
- 論理CPU上ではプロセスを順番にラウンドロビン方式で実行している
- 各プロセスのタイムラプスは大体等しい
- 全プロセス終了までの所要時間はプロセス数に比例して増加する


## コンテキストスイッチ

論理CPU上で動作するプロセスが切り替わるときにはコンテキストスイッチという処理が行なわれる．


## プロセスの状態

プロセスには次のような状態がある（網羅しているわけではないが，以下が典型的）：

- **実行状態**： 現在論理CPUを使っている
- **実行待ち状態**： CPU時間が割り当てられるのを待機している
- **スリープ状態**： 何らかのイベントが発生するのを待機しており，イベント発生まではCPU時間を使わない
  * イベントとはキーボードなどによるユーザ入力とか，タイマーの発火とか，ディスクへの書き込み完了とか，ネットワークを介した送受信の完了といったもの．
- **ゾンビ状態**： プロセスとして終了した後であり，親プロセスが終了状態を受け取るのを待っている

実際に `ps ax | wc -l` を叩くとわかるが，何も起動していないように見えても実際には300件くらいのプロセスが動作している．上記の実験で他のプロセスの影響がなかったのは，それらがスリープ状態だったため．各プロセスがどの状態なのかは `ps ax` の出力結果の3列目を見るとわかる．

- `R`： 実行状態/実行待ち状態．
- `S` または `D`； スリープ状態．後者はシグナルによって実行状態に戻ったりはしないもので，ディスクへのアクセスなどが該当する．
- `Z`： ゾンビ状態．


## 状態遷移

プロセスは或る意味では “有限状態機械” で，以下のように状態遷移する：

```
開始
 |      CPU実行権を得る
 |       +--------+
 |       |        |
 V       |        V       終了処理を呼ぶ             親プロセスが終了
[実行可能状態]     [実行状態]------------->[ゾンビ状態]-------------->[終了]
 ^       ^        |    |
 |       |        |    |
 |       +--------+    |
 |      CPU実行権を失う  |
 |                     |
 +----[スリープ状態]-----+
イベント発生      イベント待ち
```


## アイドル状態

論理CPU上でどのプロセスも実行状態でない期間は，**アイドルプロセス** という何もしない特殊なプロセスが動作する．このプロセスはCPUの特殊な命令を用いて論理CPUを休止状態（**アイドル状態**）にし，休止状態のCPUは1つ以上のプロセスが実行可能状態になるまで待機する．なんのプログラムも動かしていないと消費電力が抑えられるのはこのため．

論理CPUが単位時間あたりでどの程度アイドル状態にあったかは `sar -P ALL 1` などで確認できる．実際，`while (1)` などで無限ループするプログラムを動かすとアイドル状態が0になることが実験的に確かめられる．


## スループットとレイテンシ

- **スループット**： 単位時間あたりの総仕事量（高いほど良い）
- **レイテンシ**： 各処理の開始から終了までの経過時間（短いほど良い）

演繹的には明らかに以下が言える：

- 全ての論理CPUがアイドル状態にならない状況下では，プロセスを増やしてもスループットは変わらない．
- プロセスを増やすほどレイテンシは悪化する．


## 実際のシステム

スループットとレイテンシはトレードオフになりがち．プログラムを書くときは，望ましい目標を定義し，`sar` を使って測定することでチューニングしたりする．


## 論理CPUが複数の場合のスケジューリング

論理CPUが複数ある場合は，**ロードバランサ** または **グローバルスケジューラ** と呼ばれる機能が動作し，各論理CPUにプロセスを公平に分配する役割を担う．


## 実験方法

論理CPUが複数ある場合の動作は，**`taskset` コマンド** で実際にCPUを指定して行える．これは内部で `sched_setaffinity()` というプロセスを特定の論理CPUに制限するシステムコールを呼んでいる．


## 実験結果

2つのプロセスを2つの論理CPUで走らせると，それぞれ真に同時に走っていることが確かめられる．また，4つのプロセスを2つの論理CPUで走らせると，各論理CPUには2つずつプロセスが割り振られ，独立に両CPU上でそれぞれ2つのプロセスが切り替わりながら走る様子もわかる．


## 優先度の変更

OSは原則としてプロセスに対し平等にスケジューリングを行なうが，特定のプロセスの実行の優先度を高く設定することもでき，そのために **`nice()`** というシステムコールが用意されている．これはプロセスに−19から20までの整数で実行優先度を与えるもので，デフォルトは0であり，大きいほど優先度が高い．優先度が高いプロセスほどCPU時間を多く割り振るようにスケジュールされる．優先度を下げるのは権限に制約がないが，優先度を上げるにはroot権限を要する．


# 5章 メモリ管理

Linuxはシステムに搭載されている全メモリをカーネルの **メモリ管理システム** と呼ばれる機能により管理する．


## メモリに関する統計情報

**`free` コマンド** により，システムが搭載しているメモリの量と現在使用中のメモリの量を見ることができる．出力は以下のような項目をもつ：

- `total` フィールド： システムが搭載している全メモリ量．
- `free` フィールド： 見かけ上の空きメモリ量
- `buff/cache` フィールド： **バッファキャッシュ** と **ページキャッシュ**（6章参照）が使っているメモリ量．これらは空きメモリが減ってきたらカーネルによって解放される．
- `available` フィールド： 実質的な空きメモリ量．`free` の値に解放可能なカーネル内メモリ領域の量を足したもの．


## Out Of Memory

メモリ使用量が増えてくると，メモリ管理システムはまずカーネル内の解放可能なメモリ領域を解放する．その後もメモリ使用量が増えてどうしてもメモリが確保できなくなった場合，OSは **Out Of Memory**（**OOM**）という状態になる．この状態に至ると，メモリ管理システムは適切にプロセスを選んで強制終了することでメモリを解放するという最終手段を取る．

業務用のサーバなどでは，OOMが生じたときにどのプロセスが強制終了させられたかわからない状態で稼働し続けるのは困るので，OOMが発生するとシステムを強制終了するという設定にすることがある（`sysctl` の `vm.panic_on_oom` パラメータを `1` に設定すると可能）．


## 単純なメモリ割り当て

Linuxでのメモリ割り当ての仕組みには仮想記憶というものが欠かせないが，ここでは仮想記憶を加味しない単純な仕組みを説明し，その問題点を挙げる（この問題点を解決するのがまさに仮想記憶）．

カーネルがプロセスにメモリを割り当てるのは以下の2つのタイミングがある：

- プロセス生成時（これは3章で見た）
- プロセス生成後，追加で動的に確保するとき（これがこの章の内容）

プロセスは，追加でメモリが必要になったらメモリ獲得用のシステムコールを発行することでカーネルに確保を要請する．カーネルはそれをうけて必要な量のメモリを割り当てて，その先頭アドレスを返す．

この単純な方法には以下のような問題点がある：

- 断片化が進むと新たな確保に支障が出る
- 別用途のメモリにアクセスできてしまう
  * メモリのアドレスが丸裸なので，プロセスが自分用でないメモリにアクセスできてしまうなど抽象化に漏れがある．特にカーネルのメモリ領域にアクセスできるとまずい．
- マルチプロセスの扱いが困難
  * 各プログラムに於いてコードが何番地，データが何番地と決まった位置にあることを想定するように書かれていると，同じプログラムでもうひとつプロセスを立ち上げたりすると不整合が起きる．2種類以上のプログラムを共存させる場合も，使用するメモリの番地が重複するとやはり不整合が起きる．


## 仮想記憶

前節の問題点を解消するため，現代的なCPUには **仮想記憶** という仕組みが備わっており，これによりプロセスから見える **仮想アドレス** を実際の物理的なメモリの番地である **物理アドレス** にマップすることで抽象化をはかっている．プロセスから直接物理アドレスにアクセスする方法は存在しないようになっている．

このアドレス変換は実際にはカーネルが使うメモリ内に保持されている **ページテーブル** という表を引くことで行なう（CPUがカーネルを介さずに直接見る）．仮想記憶ではメモリはページ単位で管理されており，アドレス変換もページ単位で行なわれる．ページテーブル中の1つのページに対応するデータを **ページテーブルエントリ** と呼ぶ．ページテーブルの大きさはアーキテクチャごとに異なり，例えばx86\_64では4KB．

ページテーブルはページ単位で仮想アドレスごとにそれに紐づく物理アドレスがどこか，或いは存在しないかを保持しており，存在しないものを引いた場合は **ページフォルト** という割り込みが発生する．これに対してはページフォルトハンドラという処理が動き，この処理の中でプロセスによるメモリアクセスが不正だったことを検知する．その後 **SIGSEGV** というシグナルによりプロセスにその旨を通知し，通常その通知を受けたプロセスは強制終了させられる．

仮想記憶により以下の恩恵が得られる：

- 仮想アドレスでは連続していても物理アドレスではバラバラなように割り振ることができるので，断片化の問題が生じない．
- 仮想アドレス空間はプロセスごとに独立に確保されるので，別のプロセスやカーネルのメモリにアクセスすることはできず抽象化に漏れがない．
  * カーネルのメモリは，実装上の都合により実は全てのプロセスの仮想アドレス空間にマップされている．ただし，これはカーネルモードで動作しているときにしか読めないようになっているため，ユーザモードからカーネルのメモリを見たり変更したりすることはできない．
- マルチプロセスの取り扱いも同様の理由により困難が生じない．


## 仮想記憶の応用

以下のような機能が仮想記憶を応用して実現されている：

- **ファイルマップ**
- **デマンドページング**
- **コピーオンライト方式** の高速なプロセス生成
- **スワップ**
- **階層型ページテーブル**
- **ヒュージページ**


## ファイルマップ

通常プロセスがファイルにアクセスするときはファイルを開いた後に `read()`，`write()`，`lseek()` などのシステムコールを発行して行なうが，これに加えてLinuxにはファイルの領域を仮想アドレス空間上にマップするという機能がある．

マップされたファイルは，メモリアクセスと同じ方法でアクセスできる．writeなどの変更が施された領域は，所定のタイミングでストレージ上のファイルに書き戻される（6章参照）．


## デマンドページング

既に述べたように，プロセスに対するメモリ割り当ては次のような段階を経る：

- カーネルが，必要な領域を物理メモリ上に確保する
- カーネルがページテーブルを設定し，仮想アドレスを物理アドレス空間に紐づける

しかし，この方法だとメモリ消費に無駄が生じる．なぜなら，プロセスが確保したメモリには当分使われなかったり或いは終了まで使われない領域もあるため．例えばglibcは `malloc()` が呼び出されるよりも前にあらかじめ一定量のメモリを確保しておいて `malloc()` が呼び出されたらそこから割り当てて仮想アドレスを返すという動作をするが，実際にユーザが `malloc()` などで確保しなかったらあらかじめ確保していた分は無駄である．これを解決するのが **デマンドページング** の仕組み．

デマンドページングは，“物理アドレスを割り当てずに仮想アドレスを確保してそれをプロセス側に通知しておき，その仮想アドレスに最初にアクセスがあったときに実際に物理アドレスを確保する” という仕組み．したがってページテーブルの要素は「未割り当て」「割り当て済みで，物理メモリも確保済み」に加えて「割り当て済みだが物理メモリは未確保」という状態をとるようになる．

実際の動作としては，CPUがまずページテーブルを見て仮想アドレスが物理アドレスと紐づいていないことを検知してページフォルトを起こす．そしてカーネルのページフォルトハンドラが仮想アドレスに物理メモリを新規に割り当ててページテーブルを書き換える．そしてユーザモードに戻って通常通り処理が継続される．

デマンドページングが行なわれている下では，メモリ確保は成功したがその仮想アドレスへの最初のアクセスのときにメモリ枯渇で物理メモリが確保できず死ぬということが起きうることに注意．

デマンドページングの挙動は，実際にメモリを確保してその後アクセスするという処理を走らせて `sar` コマンドで仮想メモリと物理メモリの使用量を見ると実験的にも確かめられる．

メモリの枯渇は，物理メモリの枯渇のほかに仮想メモリの枯渇もある．これは仮想メモリのアドレス空間の範囲いっぱいまで使い切ってしまったことに相当し，物理メモリが残っていても発生する．x86では仮想メモリのアドレス空間が4GB程度しかなかったためによく起きていた．x86\_64だと128TB程度あるので今のところまず起きない（いつかはよく起きる時代になるかも）．


## コピーオンライト

`fork()` も仮想記憶の仕組みを使って高速化されている．`fork()` の発行時には，親プロセスのメモリを全て子プロセス用にコピーするのではなく，ページテーブルだけを複製し，物理メモリは共有する（つまり，ポインタを複製しているのと同じ要領）．しかし，どちらかが書き込みを行なおうとすると共有は破綻するはず．そのため，以下のような **コピーオンライト** （**CoW**）という方法をとる：


- 最初に親プロセスも子プロセスも書き込み権限がないように設定しておく．
- 親プロセスか子プロセスのどちらかかページ中のどこかを更新しようとしたとき，以下の流れで共有を解除する：
  * ページへの書き込み権限がないため，ページフォルトが生じる．
  * CPUがカーネルモードに移行してページフォルトハンドラが動き出す．
  * ページフォルトハンドラはアクセスされたページを別の場所にコピーし，書き込もうとしたプロセスに割り当てた上で内容を書き換える．
  * 親プロセスと子プロセスのそれぞれについて，共有が解除されたページに対応するページテーブルエントリを更新する．
    + 書き込んだ側のプロセスは，新たに与えられた物理ページと紐づけた上で書き込み権限を与えられる．
    + 他方のプロセスも（ページテーブルエントリは旧来のものを指したままで変わらず）書き込み権限を与えられる．

これもやはり `fork()` には成功するが共有解除のタイミングで物理メモリ枯渇による失敗が生じうることに注意．


## スワップ

物理メモリが枯渇するとOOMという状態になると既に述べたが，実際には救済措置として **スワップ** という機能がある．これはストレージデバイスをメモリの一部として一時的に使用する仕組み．枯渇が発生しそうになったら，メモリの一部をストレージに退避させることによってメモリ枯渇を防ぐ．このときにメモリを退避するためのストレージの領域を **スワップ領域** と呼ぶ．

物理メモリが枯渇しそうになったとき，まずページフォルトが発生する．このとき，カーネルはページフォルトハンドラの動作として使用中の物理メモリの一部をスワップ領域に退避させる（この動作を **スワップアウト** と呼ぶ）．どの領域をスワップアウトするかは，近いうちに使わなさそうな領域を所定のアルゴリズムによって決定する（アルゴリズムの詳細にはここでは触れない）．退避したページのスワップ領域上の位置は，ページテーブルと同要領でスワップ領域管理用の表に記載される．

再び空きメモリが十分になったとき，カーネルはスワップ領域に退避させていたデータを物理メモリに戻す（この動作を **スワップイン** と呼ぶ）．

継続的にメモリ不足の場合はメモリアクセスのたびにスワップインとスワップアウトが繰り返される **スラッシング** という状態になる．この状態に陥るとシステムは大抵殆ど応答しなくなり，ハングアップしたり（真に）OOMが発生する．


## 階層型ページテーブル

ページテーブルは，実際には配列のようなフラットな構造ではなく階層化されている．ページテーブルエントリのサイズは8バイトであり，もしページテーブルがフラットだとしたら，例えばx86\_64の1プロセスあたり128TB程度の仮想アドレス空間を扱うには 8バイト × 128TB / 4KB = 256GB の巨大なメモリが1プロセスだけで必要となり，そんなはずはない．

実際のx86\_64のページテーブルの構造はもっと複雑だが，ここではより簡単なモデルとして見る．階層型ページテーブルは区間で切られたn分木の構造をしており，物理アドレスに割り当たっている領域を含む区間はより細かい区間に分かれた下位のページテーブルを再帰的に保持する構造をとる．物理アドレスが割り当てられた仮想アドレスがその区間にひとつもない場合は下位のページテーブルが保持されない仕組みになっており，それゆえに（空間に対して実際に使っている範囲が疎であるような典型的状況では）アドレス空間の広さに対して実際に占有するデータは小さく抑えられている．

実際に使用しているページテーブルのメモリ量は `sar -r ALL` の出力の `kbpgtbl` フィールドで見ることができる．


## ヒュージページ

前節で見たように，プロセスのメモリ使用量が増えるとページテーブルもそれに伴って大きくなる．それゆえ，メモリ使用量の大きいプロセスは `fork()` にも時間がかかるようになる（親プロセスが使用している “本来のメモリの内容” 自体はコピーしないが，ページテーブルのコピーは発生するため）．これを解決する仕組みとして **ヒュージページ** という仕組みがある．

ヒュージページはその単純な名前の通り巨大なページテーブルで，階層型ページテーブルの密になった部分の記録方法を “大きい区間を大きい区間にそのままマップする” という方式にすることでエントリの個数を節約するもの．

Linuxにはさらに **トランスペアレントヒュージページ** という仕組みがあり，これは仮想アドレス空間内の連続する複数のページが所定の条件を満たした時に自動でそれらをまとめてヒュージページ化してくれるというもの．ただし，条件を満たしたとき・満たさなくなったときにヒュージページの組み立てと分解が発生して局所的にオーバーヘッドがかかるというデメリットもあり，システムによっては構築時にトランスペアレントヒュージページを無効に設定することもある．


# 6章 記憶階層

コンピュータの記憶装置には

- レジスタ
- キャッシュメモリ
- メモリ
- ストレージデバイス

という階層があり，手前ほどサイズが小さく，容量あたりの価格が高く，アクセス速度が速い．この各階層について，どの程度サイズや性能に差があるのか，これらの差を勘案してハードウェアやLinuxはどのような仕組みを実現しているのかにこの章で触れる．


## キャッシュメモリ

一般に計算機の動作は以下のような過程を経る：

1. 命令をもとにメモリからレジスタにデータを読み出す．
   - 本当はこの命令自体の読み出しもメモリから行なわれる．
2. レジスタに入ったデータをもとに計算を行なう．
3. 計算結果をメモリに書き戻す．

近年のハードウェアはレジスタ上での計算（1ナノ秒未満）に比べメモリアクセスの所要時間（数十ナノ秒）はずっと長い．したがってどれだけ計算はメモリアクセスが律速となる．これを緩和する機構が **キャッシュメモリ**．

キャッシュメモリからレジスタへの読み出しにかかる時間はメモリのそれに比べて数倍から数十倍高速であり，これを利用して1と3のフェーズを高速化できる．

キャッシュメモリは通常CPUに内蔵されるが，外部についているものもある．

キャッシュメモリの動作は，カーネルは関与せずにハードウェア内で完結している．

メモリからレジスタにデータを読み出す際に，キャッシュメモリにも同じデータとそのアドレスを読み出しておく．このあとCPUが同じアドレスのデータを読み出す際にはメモリではなくキャッシュメモリを見ればよい．

キャッシュメモリの要素を **キャッシュライン** と呼ぶ．読み出されるデータの大きさはCPUごとに決まっており，**キャッシュラインサイズ** と呼ばれる．

一方，書き込みの場合もやはりキャッシュメモリに対応するアドレスが読み出されている場合はその内容を書き換える．そしてそのキャッシュラインには「メモリから読み出された後に書き換えが行なわれた」という印をつけておく．この印がついているキャッシュラインは **ダーティ** であるという．呼ぼれているキャッシュラインは，書き込みを行なった以降の所定のタイミングでバックグラウンド処理としてメモリに書き戻され，印が取り除かれる（これを **ライトバック方式** と呼ぶ）．これにより書き込み単体はキャッシュメモリへのアクセスだけの所要時間で済む．


## キャッシュメモリがいっぱいになったら

キャッシュメモリの全てのキャッシュラインが埋まった場合，キャッシュされていないアドレスに対して読み書きを行なうと，既存のキャッシュラインのうちの1つが破棄され，そこに新しい内容が読み出される．破棄することにしたキャッシュラインがダーティな場合は，その内容をメモリに書き戻してから破棄する．ダーティなキャッシュラインを破棄するのは比較的望ましいことではなく，特に全てのキャッシュラインが継続的にダーティな状況が続くとやはりスラッシングが発生し，キャッシュメモリの性能は大幅に低下する．


## 階層型キャッシュメモリ

x86\_64ではキャッシュメモリも階層型の構造をとり，各階層によってサイズ，レイテンシ，どの論理CPUで共有するかなどが異なっている．各階層は **L1**，**L2**，**L3** などと呼ばれる（“L” は “level” の頭文字）．どのレベルまであるかはCPUに依存する．最もレジスタ寄りでサイズが小さく高速なのがL1キャッシュで，レベルが大きいほどレジスタから遠く，サイズが大きく，低速になる．

実際のマシンでは，キャッシュメモリの情報は `/sys/devices/system/cpu/cpu〈N〉/cache/index〈M〉/` といった形のディレクトリにあるファイルの中身を見ると確かめることができる．


## キャッシュの実験

プロセスがアクセスするデータのサイズによってアクセスにかかる時間がどのように変化するかを実験するとキャッシュメモリが機能している様子が確かめられる．以下のような挙動をするプログラムを書いて走らせる：

1. 第1引数に指定されたキロバイト数のメモリを確保する．
2. 確保したメモリ領域内に所定の回数だけシーケンシャルにアクセスする．
3. 1アクセスあたりの所要時間を表示する．

コンパイルは `-O3` オプションをつけて行なう．

結果を見ると，階段状に跳ね上がる箇所があり，たしかにキャッシュメモリに収まる量は一度に取得してキャッシュメモリに対するアクセスが行なわれていそうなことが確かめられる．


## 参照の局所性

実験用につくったプログラムに限らず実際的なプログラムでもキャッシュメモリは性能を向上させることが多い．というのも，プログラムは典型的には以下のような性質をもつため：

- **時間的局所性**： 或る時点でアクセスされたデータは，近い将来に再びアクセスされやすい．
  + 特にループ処理のコード領域など．
- **空間的局所性**： 或る時点で或るアドレスのデータにアクセスされると，近い将来にそれに近いアドレスのデータにアクセスすることが多い．
  + 特に配列の走査など．


## まとめ

データ配置やアルゴリズムを工夫して単位時間あたりのメモリアクセス範囲を狭くすることでキャッシュメモリの効果を活かし高速な動作を実現できる．一方で，システムの変更などによってプログラムの性能が大幅に落ちる場合，局所的なメモリアクセス範囲がキャッシュメモリに乗り切らなくなったことが疑える．


## Translation Lookaside Buffer

ここまでに述べたキャッシュの機構は物理アドレスに関するもの．

ところでプロセスが所定の仮想アドレスのデータにアクセスするには以下の手順を踏む必要があった：

1. 物理メモリ上にあるページテーブルを参照し，仮想アドレスを物理アドレスに変換する
2. 物理アドレスを用いてアクセスする

ということは，キャッシュ機構によって性能が向上できるのは2の段階だけで，1の段階は（ページテーブルがキャッシュメモリではなくメモリ上にあることから）高速化できていない．これではキャッシュメモリの利点があまり行かせていないことになる．

この問題の解決のために **translation lookaside buffer**（**TLB**）という機構がある．これは簡単にいえば “ページテーブル用のキャッシュメモリ”．詳細は触れないが，存在は知っておくとよい．


## ページキャッシュ

CPUからメモリへのアクセス速度に対し，ストレージへのアクセス速度はさらに桁違いに遅い．この差を埋めるのに使われるのが **ページキャッシュ** という仕組み．

ページキャッシュはやはりキャッシュメモリとよく似た機構で，すなわちページ単位でストレージの内容をキャッシュしておく仕組み．プロセスがファイルのデータを読み出したいと言った場合，カーネルはプロセスのメモリにファイルのデータを直接コピーするのではなく，一旦カーネルのメモリ上にあるページキャッシュの領域にコピーしてからそのデータをプロセスのメモリにコピーする．カーネルはページキャッシュにどのファイルのどの範囲を読み出したかをカーネルのメモリ内に保持する管理領域を持っている．以後，該当ファイルの該当範囲内を読み出すとき，カーネルはページキャッシュからデータを読み出す．これによりファイルへのアクセスが実際にストレージに読みに行くよりもずっと高速に終わる．ページキャッシュは全プロセスの共有資源であり，どのプロセスからの読み出しにも対処できる．

プロセスからの書き込みの要請に対しても，カーネルはやはりページキャッシュにのみデータを書き込む．そして実際のストレージにまだ書き戻されていないことを表す印をつける．この印がついたページキャッシュ内のページのことを **ダーティページ** と呼ぶ．ダーティページの内容は，やはり所定のタイミングでバックグラウンド処理として実際のストレージへと書き戻され，ダーティページであるという印もそのとき消される．

ページキャッシュは，メモリ上に空きがある限り，新たなファイルの範囲にアクセスが生じるたびに増えていき，メモリが足りなくなってきたときにカーネルによって解放される．このときはまずダーティでないページを解放し，それでも足りないならダーティページを書き戻した上で解放する．ダーティページを破棄しなければならない場合はストレージへのアクセスが発生するので性能が悪化しがちである．このダーティページの書き戻し多発によりシステムがハングアップする例はかなり多いらしい．


## 同期書き込み

ダーティページの内容はまだストレージに書き込まれていない揮発性の内容なので，書き戻しよりも前に電源遮断が発生したりすると失われる．このようなリスクを許容できない場合は，そのような書き込みが失われてはいけないファイルを `open()` システムコールで開くときに `O_SYNC` フラグという特別な設定を加える．これにより，`write()` システムコールを発行するごとに，ページキャッシュへの書き込みだけでなく実際のストレージへの書き戻しも同期的に行なわれる．


## バッファキャッシュ

ページキャッシュと似た仕組みとして **バッファキャッシュ** というものがあり，これはファイルシステムを介さずに後述の **デバイスファイル** という機構を用いてストレージデバイスに直接アクセスする際に使う（7章参照）．基本的にはページキャッシュとこのバッファキャッシュがストレージ内のデータをメモリ上に置いておく機構と考えてよい．


## ファイル読み出しの実験

ページキャッシュが機能していることを確かめるには，同じ（新しい）ファイルに2回アクセスしてそれぞれの所要時間を測ったり，`free` コマンドや `sar -r` を使ってファイル読み出し前後でのページキャッシュ使用量を確認したりするとよい．

著者の環境の実験では，1GB程度のファイルを作成して読み出すと，1回目は約2秒かかり，その直後実際にページキャッシュが1GB程度増えていた，そして2回目の読み出しを行なうと約0.1秒で終了した．

また，実際にファイル作成，読み出し1回目，読み出し2回目をシェルスクリプトで動作させながら `sar -B` をバックグラウンドで動かして時系列を見る．すると，最初のファイル作成時に合計1GBのページアウトが発生し，読み出し1回目でページインが発生，読み出し2回目では（システム上の他の処理で生じるわずかな変化を除いて）ページインは発生しない．ストレージデバイスのI/Oも `sar -d -p` で見ることができ，これをみるとたしかにファイル作成時に書き込みが，読み出し1回目のみで読み出しが発生していることがわかる．


## ファイル書き込みの実験

書き込みも同様にページキャッシュにより性能が上がり，また書き込みの間ページアウトが発生しないことも確認できる．


## チューニングパラメータ

Linuxにはページキャッシュを制御するためのパラメータがあり，`sysctl` で設定できる．以下がその一部：

- `vm.dirty_writeback_centisecs`： ダーティページの書き戻しの周期．デフォルトでは500センチ秒，すなわち5秒．
- `vm.dirty_background_ratio`： 全物理メモリに対してダーティページがこれに指定された割合（パーセント単位）を超えた場合は書き戻しを行なう．
- `vm.dirty_background_bytes`： 上の項目のバイト単位版．0は未指定扱い．
- `vm.dirty_ratio`： ダーティページの割合がこれに指定された割合を超えた場合はプロセスによるファイル書き込みの際に同期的に書き戻しを行なう．
- `vm.dirty_bytes`： 上の項目のバイト単位版．0は未指定扱い．


## ハイパースレッド

CPUによる計算時間よりもメモリアクセスの方がずっと遅いため，一般にプロセスの動作はメモリによって律速されており，CPUは時間軸上結構高い割合で待機している．ちなみにメモリアクセスのほかにもCPU資源を空費してしまう理由はいくつもある．

待ち時間で空費されがちなCPU資源を有効活用するのが **ハイパースレッド機能**．これは，1個の物理的なCPUに対してレジスタなどの資源を複数（大抵は2つずつ）用意して，システムからは複数の論理CPUとして認識されるようにしたもの．このときの論理CPUを **ハイパースレッド** と呼ぶ．要するに “片側が待ち時間の場合は他方のCPUとして使う” という仕組み．排他制御を要する以上，2つのハイパースレッドに分割すれば2倍くらい良くなるかというと別にそうでもなく，現実的には2〜3割ほど向上すれば良い方．利用することでむしろ遅くなることもある．

手元の環境でどの論理CPUの組が物理的なCPUを共有しているかは `/sys/devices/system/cpu/cpu〈CPU番号〉/topology/thread_siblings_list` を見るとわかる．


# 7章 ファイルシステム

## Linuxのファイルシステム

Linuxがサポートするファイルシステムの実装は **ext4**，**XFS**，**Btrfs** などいろいろあるが，いずれもユーザからはシステムコールの発行という共通のインターフェイスによって隠蔽されている：

- 作成，削除： `creat()`，`unlink()`
- 開く，閉じる： `open()`，`close()`
- 開いたファイルからの読み出し： `read()`
- 開いたファイルへの書き込み： `write()`
- 開いたファイルを所定の位置に移動： `lseek()`
- ファイルシステム依存の特殊処理： `ioctl()`

これらのシステムコールが発行されると，以下のような手順でデータが読み書きされる：

1. カーネル内の全ファイルシステムに共通の処理が動作し，操作対象のファイルシステムが判別される．
2. 各ファイルシステムを扱う処理を呼び出して，システムコールごとの処理を行なう．
3. データの読み書きをするなら，デバイスドライバに処理を依頼する．
4. デバイスドライバがデータを読み書きする．
